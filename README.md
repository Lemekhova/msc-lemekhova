# Исследование методов эффективного применения адаптеров (LoRA)  для дообучения моделей в условиях ограниченных вычислительных ресурсов
1. Введение

1.1. Актуальность применения языковых моделей

Рост параметров современных языковы моделей

Ограниченност вычислительных ресурсов

Необходимость применения методов parameter-efficient fine-tuning

Ограниченность объёма размеченных данных

1.2. Цель и задачи исследования

Цель — исследовать влияние параметров LoRA и объёма обучающих данных на качество дообучения языковых моделей в условиях ограниченных вычислительных ресурсов.

Задачи:

проанализировать современные подходы к дообучению языковых моделей

реализовать baseline с полным fine-tuning

реализовать дообучение с использованием LoRA

исследовать влияние параметров LoRA

исследовать влияние квантизации

проанализировать влияние объёма обучающих данных

сравнить подходы по: качеству генерации, потреблению видеопамяти (VRAM), latency инференса

обеспечить воспроизводимость экспериментальных результатов

1.3. Объект и предмет исследования

1.4. Научная новизна и практическая значимость

1.5. Структура работы

2. Обзор литературы и теоретические основы

2.1. Архитектура трансформеров в языковых моделях

2.2. Подходы к дообучению языковых моделей

2.3.  Parameter-Efficient Fine-Tuning

2.4. Теоретические основы метода LoRA

2.5. Квантизация языковых моделей

2.6  Выводы

3. Анализ существующих решенийи и постановка задачи

3.1. Open-source библиотеки для дообучения LLM

3.2. Сравнение подходов к дообучению

3.3. Формулировка исследовательских гипотез

3.4. Метрики оценки

Влияние LoRA при малом объёме данных

Влияние ранга и target modules

Влияние квантизации на качество и ресурсы

3.4. Ограничения экспериментов

4. Подготовка и анализ данных
   
4.1. Постановка NLP-задачи
     
4.2. Описание и источники данных

4.3. Сбор, очистка и предобработка данных

4.4. Формирование обучающих выборок

4.5. EDA

5. Построение baseline-модели
   
5.1. Выбор базовой языковой модели

5.2. Полное дообучение модели (Full Fine-Tuning) - Пункт под вопросом ?
     
Архитектура
     
Настройки обучения
     
Ограничения по ресурсам
     
5.3. Метрики качества

5.4. Результаты baseline

6. Дообучение языковой модели с использованием LoRA и квантизации
   
6.1. Интеграция LoRA в архитектуру модели

6.2. Исследуемые параметры LoRA

6.3. Методы квантизации

6.3. Экспериментальная схема обучения

6.4. Эксперименты

7. Анализ и визуализация результатов (дашборд?)

7.1. Сравнение LoRA и Full Fine-Tuning

7.2. Влияние параметров LoRA на качество

7.3. Влияние квантизации на качество и ресурсы

7.4. Анализ зависимости от объёма данных

7.5. Дашборд экспериментов ?

7.6. Анализ «качество – ресурсы»

8. Обеспечение воспроизводимости экспериментов
    
8.1. Фиксация программного окружения

8.2. Контейнеризация экспериментов с использованием Docker

8.3. Повторный запуск и верификация результатов

9. Заключение

10. Список литературы
    
11. Приложения
    
Dockerfile

Конфигурации экспериментов - experiments.ipynb

Скрипты

Ссылки на репозиторий:
[experiments.ipynb](https://github.com/Lemekhova/msc-lemekhova/blob/main/experiments.ipynb)
